import logging

import telebot
from langchain.chains import LLMChain
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq

groq_api_key = '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'
telegram_bot_api_key = '7072266877:AAEzRwuHbELQ_EeiGIku4n9MBdcui_9b3lQ'
model = 'llama3-8b-8192'
conversational_memory_length = 3
bot_default_role = 'полезный, уважительный и честный помощник'
system_message = (f"Я всегда отвечаю на русском языке. Я {bot_default_role}. "
                  f"Я всегда отвечаю максимально полезно. "
                  f"Если вопрос не имеет никакого смысла или мне не понятен, я объясняю, почему, "
                  f"вместо того, чтобы ответить на что-то неправильное. "
                  f"Если я не знаю ответ на вопрос, я не даю ложную информацию, "
                  f"никогда не вру и честно признаюсь, что не знаю ответа.")
users = {}

bot = telebot.TeleBot(telegram_bot_api_key)

logging.basicConfig(level=logging.INFO)


# def start(update, context):
#     context.bot.send_message(chat_id=update.effective_chat.id,
#                              text="Привет! Я ваш дружелюбный чат-бот gerrich. "
#                                   "Я могу помочь ответить на ваши вопросы, "
#                                   "предоставить информацию или просто пообщаться. "
#                                   "Я также очень шустрый! Давайте начнем наш разговор!")


@bot.message_handler(content_types=['text'])
def get_text_messages(message):
    user_question = message.text
    chat_id = message.from_user.id

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history",
                                            return_messages=True)

    # session state variable
    if chat_id not in users:  # new user
        users[chat_id] = {'chat_history': []}
    else:
        if len(users[chat_id]['chat_history']) > conversational_memory_length:
            users[chat_id]['chat_history'] = users[chat_id]['chat_history'][-conversational_memory_length:]

        for message in users[chat_id]['chat_history']:
            memory.save_context(
                {'input': message['human']},
                {'output': message['AI']}
            )

    groq_chat = ChatGroq(
        groq_api_key=groq_api_key,
        model_name=model
    )

    # Construct a chat prompt template using various components
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage(
                content=system_message
            ),
            MessagesPlaceholder(
                variable_name="chat_history"
            ),
            HumanMessagePromptTemplate.from_template(
                "{human_input}"
            ),
        ]
    )

    # Create a conversation chain using the LangChain LLM (Language Learning Model)
    conversation = LLMChain(
        llm=groq_chat,
        prompt=prompt,
        verbose=True,
        memory=memory,
    )

    # The chatbot's answer is generated by sending the full prompt to the Groq API.
    response = conversation.predict(human_input=user_question)
    message = {'human': user_question, 'AI': response}
    users[chat_id]['chat_history'].append(message)

    bot.send_message(chat_id, response)


while True:
    bot.polling(none_stop=True, interval=0, timeout=0)
